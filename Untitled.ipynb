{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import base64\n",
    "import random\n",
    "import chardet\n",
    "import codecs\n",
    "from re import sub\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "# Либо стемер\n",
    "# from nltk.stem import SnowballStemmer\n",
    "#stemmer = SnowballStemmer(\"russian\");\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "from collections import defaultdict, Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer = TweetTokenizer()\n",
    "\n",
    "class SpamHTMLParser(HTMLParser):\n",
    "    def __init__(self):\n",
    "        HTMLParser.__init__(self)\n",
    "        self.__text = \"\"\n",
    "        self.title_len = 0\n",
    "        self.a_len = 0\n",
    "        self.is_title = False\n",
    "        self.is_a = False\n",
    "        \n",
    "        self.a_count = 0\n",
    "        \n",
    "        self.is_em = False\n",
    "        self.em_count = 0\n",
    "        \n",
    "        self.script_count = 0\n",
    "        \n",
    "        self.a_counter = Counter()\n",
    "        \n",
    "\n",
    "    def handle_data(self, data):\n",
    "        text = data.strip()\n",
    "        if len(text) > 0:\n",
    "            text = sub('[ \\t\\r\\n]+', ' ', text)\n",
    "            self.__text += text + ' '\n",
    "            \n",
    "        if self.is_title:\n",
    "            self.title_len = len(text.split(' '))\n",
    "            \n",
    "        if self.is_a:\n",
    "#             text = text.split(' ')\n",
    "            \n",
    "            a_tokens = tokenizer.tokenize(text)\n",
    "            self.a_len += len(a_tokens)\n",
    "            self.a_counter.update(a_tokens)\n",
    "\n",
    "    def handle_starttag(self, tag, attrs):\n",
    "        if tag == 'title':\n",
    "            self.is_title = True\n",
    "            \n",
    "        if tag == 'script':\n",
    "            self.script_count += 1\n",
    "            \n",
    "        if tag == 'a':\n",
    "            self.is_a = True\n",
    "            self.a_count += 1\n",
    "            \n",
    "        if tag == 'em':\n",
    "            self.is_em = True\n",
    "            self.em_count += 1\n",
    "        \n",
    "        if tag == 'p':\n",
    "            self.__text += '\\n\\n'\n",
    "        elif tag == 'br':\n",
    "            self.__text += '\\n'\n",
    "            \n",
    "            \n",
    "#     def handle_data(self, data):\n",
    "        \n",
    "            \n",
    "    def handle_endtag(self, tag):\n",
    "        if tag == 'title':\n",
    "            self.is_title = False\n",
    "            \n",
    "        if tag == 'a':\n",
    "            self.is_a = False\n",
    "            \n",
    "        if tag == 'em':\n",
    "            self.is_em = False\n",
    "\n",
    "    def handle_startendtag(self, tag, attrs):\n",
    "        \n",
    "            \n",
    "        if tag == 'a':\n",
    "            self.is_a = False\n",
    "            \n",
    "        if tag == 'br':\n",
    "            self.__text  += '\\n\\n'\n",
    "            \n",
    "    def get_stat(self):\n",
    "        res = dict(\n",
    "            title_len = self.title_len,\n",
    "            a_len = self.a_len * 1.0 / (self.a_count or 1.0),\n",
    "            a_count = self.a_count,\n",
    "            script_count = self.script_count,\n",
    "            a_unique_words = len(self.a_counter),\n",
    "            \n",
    "        )\n",
    "        \n",
    "        return res\n",
    "        \n",
    "\n",
    "    def text(self):\n",
    "        return ''.join(self.__text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def read_file(DATA_FILE):\n",
    "    res = []\n",
    "\n",
    "    with open (DATA_FILE) as df:\n",
    "        doc_cnt = 0\n",
    "        for doc_cnt, line in enumerate(df):\n",
    "            line = line.strip()\n",
    "            # делим по \\t\n",
    "            parts = line.split()\n",
    "            # класс примера 0 || 1\n",
    "            item_class = int(parts[1])\n",
    "            url = parts[2]\n",
    "            pageInb64 = parts[3]\n",
    "\n",
    "            html = base64.b64decode(pageInb64).decode('utf-8') \n",
    "\n",
    "            parser = SpamHTMLParser()\n",
    "\n",
    "            parser.feed(html)\n",
    "            text = parser.text()\n",
    "\n",
    "            #\n",
    "            # возвращает токенизированные предложения\n",
    "            #\n",
    "            \n",
    "            if len(parser.text()) > 1000:\n",
    "                tokens = parser.text().split(' ')\n",
    "            else:\n",
    "                tokens = tokenizer.tokenize(parser.text())\n",
    "            \n",
    "            \n",
    "#             x.append(tokens)\n",
    "#             y.append(item_class)\n",
    "            stat = parser.get_stat()\n",
    "    \n",
    "            stat[\"word_len\"] = reduce(lambda acc, w: len(w) * 1.0 + acc, tokens, 0.0)/len(tokens)\n",
    "            stat[\"word_cnt\"] = len(tokens)\n",
    "            words = Counter(tokens)\n",
    "            \n",
    "            stat[\"a_unique_words_ratio\"] = stat[\"a_unique_words\"] * 1.0/len(words)\n",
    "            stat[\"unique_words_ratio\"] = len(words) * 1.0/len(tokens)\n",
    "            \n",
    "#             sys.stdout.write('\\r' + \"%s\" % (len(words))) \n",
    "            \n",
    "            for i, (word, cnt) in enumerate(words.most_common(10)):\n",
    "                stat[\"word_{0}_rate\".format(i)] = cnt * 1.0/len(tokens)\n",
    "            \n",
    "            \n",
    "            res.append((tokens, url, item_class, stat))\n",
    "            sys.stdout.write('\\r' + \"%s\" % (doc_cnt))  \n",
    "            sys.stdout.flush()\n",
    "            \n",
    "#             if doc_cnt > 1150:\n",
    "#                 break\n",
    "            \n",
    "        return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043"
     ]
    }
   ],
   "source": [
    "DATA_FILE  = './data/train-set-ru-b64-utf-8.txt'\n",
    "data = read_file(DATA_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_datasets(data, cv = 0):\n",
    "    raw_x = map(lambda item: item[3], data)\n",
    "    y = np.array(map(lambda item: item[2], data)) \n",
    "    dict_vectorizer = DictVectorizer(sparse=False)\n",
    "    x = dict_vectorizer.fit_transform(raw_x)\n",
    "    \n",
    "    n = len(x)\n",
    "    indexes = np.random.permutation(len(x))\n",
    "    \n",
    "    x_train = x[indexes[:n - cv]]\n",
    "    y_train = y[indexes[:n - cv]]\n",
    "    \n",
    "    x_test = x[indexes[n - cv:]]\n",
    "    y_test = y[indexes[n - cv:]] \n",
    "    \n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = get_datasets(data, cv=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gb = gb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pred = gb.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91190476190476188"
      ]
     },
     "execution_count": 241,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
