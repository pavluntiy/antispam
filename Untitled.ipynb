{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import base64\n",
    "import random\n",
    "import chardet\n",
    "import codecs\n",
    "from re import sub\n",
    "from collections import defaultdict\n",
    "\n",
    "from nltk.stem import SnowballStemmer\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import zlib\n",
    "\n",
    "stemmer = SnowballStemmer(\"russian\")\n",
    "\n",
    "import pymorphy2\n",
    "\n",
    "# Либо стемер\n",
    "# from nltk.stem import SnowballStemmer\n",
    "#stemmer = SnowballStemmer(\"russian\");\n",
    "\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "\n",
    "from HTMLParser import HTMLParser\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tokenizer = TweetTokenizer()\n",
    "\n",
    "# class SpamHTMLParser(HTMLParser):\n",
    "#     def __init__(self):\n",
    "#         HTMLParser.__init__(self)\n",
    "#         self.__text = \"\"\n",
    "#         self.title_len = 0\n",
    "#         self.a_len = 0\n",
    "#         self.is_title = False\n",
    "#         self.is_a = False\n",
    "        \n",
    "#         self.a_count = 0\n",
    "        \n",
    "#         self.is_em = False\n",
    "#         self.em_count = 0\n",
    "        \n",
    "#         self.script_count = 0\n",
    "        \n",
    "#         self.a_counter = Counter()\n",
    "        \n",
    "#         self.h1_count = 0\n",
    "#         self.h2_count = 0\n",
    "#         self.h3_count = 0\n",
    "        \n",
    "#         self.table_count = 0\n",
    "# #         self.is_h1 = False\n",
    "# #         self.h1_len = 0\n",
    "        \n",
    "\n",
    "#     def handle_data(self, data):\n",
    "#         text = data.strip()\n",
    "#         if len(text) > 0:\n",
    "#             text = sub('[ \\t\\r\\n]+', ' ', text)\n",
    "#             self.__text += text + ' '\n",
    "            \n",
    "#         if self.is_title:\n",
    "#             self.title_len = len(text.split(' '))\n",
    "            \n",
    "#         if self.is_a:\n",
    "# #             text = text.split(' ')\n",
    "            \n",
    "#             a_tokens = tokenizer.tokenize(text)\n",
    "#             self.a_len += len(a_tokens)\n",
    "#             self.a_counter.update(a_tokens)\n",
    "            \n",
    "            \n",
    "\n",
    "#     def handle_starttag(self, tag, attrs):\n",
    "#         if tag == 'title':\n",
    "#             self.is_title = True\n",
    "            \n",
    "#         if tag == 'table':\n",
    "#             self.table_count += 1\n",
    "            \n",
    "#         if tag == 'script':\n",
    "#             self.script_count += 1\n",
    "            \n",
    "#         if tag == 'h1':\n",
    "#             self.h1_count += 1\n",
    "            \n",
    "#         if tag == 'h2':\n",
    "#             self.h2_count += 1\n",
    "            \n",
    "#         if tag == 'h3':\n",
    "#             self.h3_count += 1\n",
    "            \n",
    "#         if tag == 'a':\n",
    "#             self.is_a = True\n",
    "#             self.a_count += 1\n",
    "            \n",
    "#         if tag == 'em':\n",
    "#             self.is_em = True\n",
    "#             self.em_count += 1\n",
    "        \n",
    "#         if tag == 'p':\n",
    "#             self.__text += '\\n\\n'\n",
    "#         elif tag == 'br':\n",
    "#             self.__text += '\\n'\n",
    "            \n",
    "            \n",
    "# #     def handle_data(self, data):\n",
    "        \n",
    "            \n",
    "#     def handle_endtag(self, tag):\n",
    "#         if tag == 'title':\n",
    "#             self.is_title = False\n",
    "            \n",
    "#         if tag == 'a':\n",
    "#             self.is_a = False\n",
    "            \n",
    "#         if tag == 'em':\n",
    "#             self.is_em = False\n",
    "\n",
    "#     def handle_startendtag(self, tag, attrs):\n",
    "        \n",
    "            \n",
    "#         if tag == 'a':\n",
    "#             self.is_a = False\n",
    "            \n",
    "#         if tag == 'br':\n",
    "#             self.__text  += '\\n\\n'\n",
    "            \n",
    "#     def get_stat(self):\n",
    "#         res = dict(\n",
    "#             title_len = self.title_len,\n",
    "#             a_len = self.a_len * 1.0 / (self.a_count or 1.0),\n",
    "#             a_count = self.a_count,\n",
    "#             script_count = self.script_count,\n",
    "#             a_unique_words = len(self.a_counter),\n",
    "#             h1_count = self.h1_count,\n",
    "#             h2_count = self.h2_count,\n",
    "#             h3_count = self.h3_count,\n",
    "#             table_count = self.table_count\n",
    "            \n",
    "#         )\n",
    "        \n",
    "#         return res\n",
    "        \n",
    "\n",
    "#     def text(self):\n",
    "#         return ''.join(self.__text).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# def read_file(DATA_FILE):\n",
    "#     res = []\n",
    "\n",
    "#     with open (DATA_FILE) as df:\n",
    "#         doc_cnt = 0\n",
    "#         for doc_cnt, line in enumerate(df):\n",
    "#             line = line.strip()\n",
    "#             # делим по \\t\n",
    "#             parts = line.split()\n",
    "#             # класс примера 0 || 1\n",
    "#             item_class = int(parts[1])\n",
    "#             url = parts[2]\n",
    "#             pageInb64 = parts[3]\n",
    "\n",
    "#             html = base64.b64decode(pageInb64).decode('utf-8') \n",
    "\n",
    "#             parser = SpamHTMLParser()\n",
    "\n",
    "#             parser.feed(html)\n",
    "#             text = parser.text()\n",
    "\n",
    "#             #\n",
    "#             # возвращает токенизированные предложения\n",
    "#             #\n",
    "            \n",
    "            \n",
    "            \n",
    "#             if len(parser.text()) > 1000:\n",
    "#                 tokens = parser.text().split(' ')\n",
    "#             else:\n",
    "#                 tokens = tokenizer.tokenize(parser.text())\n",
    "            \n",
    "            \n",
    "# #             x.append(tokens)\n",
    "# #             y.append(item_class)\n",
    "#             stat = parser.get_stat()\n",
    "    \n",
    "#             stat[\"text_to_html_ratio\"] = 1.0 * len(text)/len(html)\n",
    "        \n",
    "#             compressed_html = zlib.compress(html.encode(\"utf-8\"))\n",
    "#             stat[\"compressed_to_html\"] = 1.0 * len(compressed_html)/len(html.encode(\"utf-8\"))\n",
    "            \n",
    "#             compressed_text = zlib.compress(text.encode(\"utf-8\"))\n",
    "#             stat[\"compressed_to_text\"] = 1.0 * len(compressed_text)/len(text.encode(\"utf-8\"))\n",
    "    \n",
    "#             stat[\"word_len\"] = reduce(lambda acc, w: len(w) * 1.0 + acc, tokens, 0.0)/len(tokens)\n",
    "# #             stat[\"word_cnt\"] = len(tokens)\n",
    "#             words = Counter(tokens)\n",
    "            \n",
    "#             stat[\"a_unique_words_ratio\"] = stat[\"a_unique_words\"] * 1.0/len(words)\n",
    "#             stat[\"unique_words_ratio\"] = len(words) * 1.0/len(tokens)\n",
    "            \n",
    "# #             sys.stdout.write('\\r' + \"%s\" % (len(words))) \n",
    "            \n",
    "#             for i, (word, cnt) in enumerate(words.most_common(5)):\n",
    "#                 stat[\"word_{0}_rate\".format(i)] = cnt * 1.0/len(tokens)\n",
    "            \n",
    "            \n",
    "#             res.append((tokens, url, item_class, stat))\n",
    "#             sys.stdout.write('\\r' + \"%s\" % (doc_cnt))  \n",
    "#             sys.stdout.flush()\n",
    "            \n",
    "# #             if doc_cnt > 100:\n",
    "# #                 break\n",
    "            \n",
    "#         return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'antispam_classifier' from 'antispam_classifier.py'>"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import antispam_classifier\n",
    "reload(antispam_classifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "DATA_FILE  = './data/train-set-ru-b64-utf-8.txt'\n",
    "# raw = antispam_classifier.read_file(DATA_FILE, max_count = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7043"
     ]
    }
   ],
   "source": [
    "data = antispam_classifier.process_docs(raw, max_count = None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp1 = {'word_cnt': 541, 'word_4_rate': 0.029574861367837338, 'word_len': 5.709796672828096, 'word_0_rate': 0.04066543438077634, 'a_unique_words': 38, 'word_1_rate': 0.038817005545286505, 'word_2_rate': 0.033271719038817, 'a_len': 2.1785714285714284, 'h2_count': 0, 'h3_count': 0, 'unique_words_ratio': 0.6820702402957486, 'a_unique_words_ratio': 0.10298102981029811, 'table_count': 12, 'script_count': 1, 'title_len': 3, 'h1_count': 1, 'word_3_rate': 0.031423290203327174, 'a_count': 28}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# set(tmp1.keys()) ^ set(tmp.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'a_av_len': 3.6595744680851063,\n",
       " 'a_unique_words': 82,\n",
       " 'a_unique_words_ratio': 0.19158878504672897,\n",
       " 'bold_count': 9,\n",
       " 'compressed_to_html': 0.3239855072463768,\n",
       " 'compressed_to_text': 0.3422973959600876,\n",
       " 'em_av_len': 3.0,\n",
       " 'em_count': 2,\n",
       " 'em_len': 6,\n",
       " 'h1_count': 8,\n",
       " 'h2_count': 0,\n",
       " 'h3_count': 0,\n",
       " 'img_count': 0,\n",
       " 'p_count': 7,\n",
       " 'script_count': 4,\n",
       " 'table_count': 0,\n",
       " 'text_len': 4708,\n",
       " 'text_to_html_ratio': 0.45990036143401386,\n",
       " 'title_len': 6,\n",
       " 'unique_words_ratio': 0.6604938271604939,\n",
       " 'word_cnt': 648,\n",
       " 'word_len': 6.2669753086419755}"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[83][1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_datasets(data, cv = 0):\n",
    "    raw_x = map(lambda item: item[1], data)\n",
    "    y = np.array(map(lambda item: item[0], data)) \n",
    "    dict_vectorizer = DictVectorizer(sparse=False)\n",
    "    \n",
    "    x = dict_vectorizer.fit_transform(raw_x)\n",
    "    \n",
    "    dd = dict_vectorizer.vocabulary_\n",
    "#     a = [dd[\"a_count\"], dd[\"compressed_to_html\"], dd[\"word_len\"], \n",
    "#          dd[\"word_cnt\"], dd[\"script_count\"], dd[\"title_len\"],\n",
    "#          dd[\"h1_count\"], dd[\"p_count\"]\n",
    "#         ]\n",
    "#     x = x[:, a]\n",
    "    feature_names =  dd.keys()\n",
    "    \n",
    "    n = len(x)\n",
    "    indexes = np.random.permutation(len(x))\n",
    "    \n",
    "    x_train = x[indexes[:n - cv]]\n",
    "    y_train = y[indexes[:n - cv]]\n",
    "    \n",
    "    x_test = x[indexes[n - cv:]]\n",
    "    y_test = y[indexes[n - cv:]] \n",
    "    \n",
    "    \n",
    "    \n",
    "    return x_train, y_train, x_test, y_test, np.array(feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test, feature_names = get_datasets(data, 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# x_train.shape\n",
    "# y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import f1_score, precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = GradientBoostingClassifier(n_estimators=2500, subsample=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "gb = gb.fit(x_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "scores = cross_val_score(gb, x_train, y_train, cv = 5, scoring='f1_macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.94587049  0.95169785  0.95040581  0.94788943  0.94446244]\n",
      "0.948065204578\n"
     ]
    }
   ],
   "source": [
    "# pred = gb.predict(x_test)\n",
    "print scores\n",
    "print scores.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# f1_score(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# tmp = "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# precision_recall_fscore_support(pred, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pickle.dump(gb, open(\"model.pickle\", \"w\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['title_len', 'compressed_to_html', 'a_count', 'h3_count', 'a_ratio',\n",
       "       'word_cnt', 'a_len', 'p_count', 'text_len', 'a_unique_words_ratio',\n",
       "       'a_unique_words', 'compressed_to_text', 'h2_count',\n",
       "       'unique_words_ratio', 'em_len', 'script_count', 'word_len',\n",
       "       'em_count', 'h1_count', 'bold_count', 'img_count',\n",
       "       'text_to_html_ratio', 'a_av_len', 'table_count', 'em_av_len'], \n",
       "      dtype='|S20')"
      ]
     },
     "execution_count": 179,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_names[(-gb.feature_importances_).argsort()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
